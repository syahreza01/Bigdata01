{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e2a030e3",
      "metadata": {
        "id": "e2a030e3"
      },
      "source": [
        "# Hands-On Pertemuan 14: Advanced Machine Learning using Spark MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099562db",
      "metadata": {
        "id": "099562db"
      },
      "source": [
        "## Objectives:\n",
        "- Understand and implement advanced machine learning tasks using Spark MLlib.\n",
        "- Build and evaluate models using real-world datasets.\n",
        "- Explore techniques like feature engineering and hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77df771a",
      "metadata": {
        "id": "77df771a"
      },
      "source": [
        "## Introduction to Spark MLlib\n",
        "Spark MLlib is a scalable library for machine learning that integrates seamlessly with the Spark ecosystem. It supports a wide range of tasks, including regression, classification, clustering, and collaborative filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d9ae225b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ae225b",
        "outputId": "94fea7d0-8268-407d-9b7e-e002b32878b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.9999999999999992]\n",
            "Intercept: 15.000000000000009\n"
          ]
        }
      ],
      "source": [
        "# Example: Linear Regression with Spark MLlib\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
        "\n",
        "# Load sample data\n",
        "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
        "columns = ['ID', 'Feature', 'Target']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
        "model = lr.fit(df_transformed)\n",
        "\n",
        "# Print model coefficients\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Iris dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -O iris.csv\n",
        "# -*- coding: utf-8 -*-\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Download the Iris dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -O iris.csv\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('Iris Classification').getOrCreate()\n",
        "\n",
        "# Load dataset (Iris dataset)\n",
        "data = spark.read.csv('iris.csv', header=False, inferSchema=True)\n",
        "\n",
        "# Rename columns for clarity\n",
        "data = data.withColumnRenamed('_c0', 'sepal_length') \\\n",
        "           .withColumnRenamed('_c1', 'sepal_width') \\\n",
        "           .withColumnRenamed('_c2', 'petal_length') \\\n",
        "           .withColumnRenamed('_c3', 'petal_width') \\\n",
        "           .withColumnRenamed('_c4', 'species')\n",
        "\n",
        "# Convert species to numeric using StringIndexer\n",
        "indexer = StringIndexer(inputCol='species', outputCol='label')\n",
        "data_indexed = indexer.fit(data).transform(data)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')\n",
        "df_transformed = assembler.transform(data_indexed)\n",
        "\n",
        "# Select only the relevant columns\n",
        "df_transformed = df_transformed.select('features', 'label')\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "# Train a logistic regression model\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
        "\n",
        "# Create a parameter grid for hyperparameter tuning\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .addGrid(lr.maxIter, [10, 20]) \\\n",
        "    .build()\n",
        "\n",
        "# Create a CrossValidator\n",
        "crossval = CrossValidator(estimator=lr,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy'),\n",
        "                          numFolds=3)  # Use 3+ folds in practice\n",
        "\n",
        "# Fit the model using cross-validation\n",
        "cvModel = crossval.fit(train_data)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = cvModel.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rusQ28rOXLrU",
        "outputId": "431bf916-d655-45b7-96f0-26e2ccad16e0"
      },
      "id": "rusQ28rOXLrU",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-10 11:39:30--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘iris.csv’\n",
            "\n",
            "iris.csv                [ <=>                ]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-10 11:39:30 (22.1 MB/s) - ‘iris.csv’ saved [4551]\n",
            "\n",
            "--2024-12-10 11:39:30--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘iris.csv’\n",
            "\n",
            "iris.csv                [ <=>                ]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-10 11:39:30 (77.0 MB/s) - ‘iris.csv’ saved [4551]\n",
            "\n",
            "Accuracy: 0.9459459459459459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('KMeans Example').getOrCreate()\n",
        "\n",
        "# Example dataset with separate features\n",
        "data = [(1, 1.0, 1.0), (2, 5.0, 5.0), (3, 10.0, 10.0), (4, 15.0, 15.0)]\n",
        "columns = ['ID', 'Feature1', 'Feature2']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for KMeans\n",
        "# Use VectorAssembler to combine the feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=['Feature1', 'Feature2'], outputCol='features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train KMeans clustering model\n",
        "kmeans = KMeans(featuresCol='features', k=2)\n",
        "model = kmeans.fit(df_transformed)\n",
        "\n",
        "# Show cluster centers\n",
        "centers = model.clusterCenters()\n",
        "print(f'Cluster Centers: {centers}')\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMa5BhMDX6uL",
        "outputId": "7039edc2-04d9-4d3c-f228-049edcfcb238"
      },
      "id": "oMa5BhMDX6uL",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60a8d7e",
      "metadata": {
        "id": "a60a8d7e"
      },
      "source": [
        "## Homework\n",
        "- Load a real-world dataset into Spark and prepare it for machine learning tasks.\n",
        "- Build a classification model using Spark MLlib and evaluate its performance.\n",
        "- Explore hyperparameter tuning using cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import urllib.request\n",
        "\n",
        "# Download the Iris dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "file_path = 'iris.data'\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "# Now, let's proceed with Spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('Iris Classification').getOrCreate()\n",
        "\n",
        "# Load dataset (Iris dataset) from local file\n",
        "data = spark.read.csv(file_path, header=False, inferSchema=True)\n",
        "\n",
        "# Rename columns for clarity\n",
        "data = data.withColumnRenamed('_c0', 'sepal_length') \\\n",
        "           .withColumnRenamed('_c1', 'sepal_width') \\\n",
        "           .withColumnRenamed('_c2', 'petal_length') \\\n",
        "           .withColumnRenamed('_c3', 'petal_width') \\\n",
        "           .withColumnRenamed('_c4', 'species')\n",
        "\n",
        "# Convert species to numeric using StringIndexer\n",
        "indexer = StringIndexer(inputCol='species', outputCol='label')\n",
        "data_indexed = indexer.fit(data).transform(data)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')\n",
        "df_transformed = assembler.transform(data_indexed)\n",
        "\n",
        "# Select only the relevant columns\n",
        "df_transformed = df_transformed.select('features', 'label')\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "# Train a logistic regression model\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
        "\n",
        "# Create a parameter grid for hyperparameter tuning\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .addGrid(lr.maxIter, [10, 20]) \\\n",
        "    .build()\n",
        "\n",
        "# Create a CrossValidator\n",
        "crossval = CrossValidator(estimator=lr,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy'),\n",
        "                          numFolds=3)  # Use 3+ folds in practice\n",
        "\n",
        "# Fit the model using cross-validation\n",
        "cvModel = crossval.fit(train_data)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = cvModel.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQDQPSrsYkg0",
        "outputId": "c4912365-46e8-452c-c706-2b118ddb93f9"
      },
      "id": "mQDQPSrsYkg0",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9459459459459459\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}